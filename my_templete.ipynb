{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kaggleコンペpetfinder　https://www.kaggle.com/c/petfinder-adoption-prediction\n",
    "をfastaiやpytorch Lightningのような\n",
    "ラッパーライブラリなしで実装したノートブック\n",
    "\n",
    "参照\n",
    "https://www.kaggle.com/yasufuminakama/petfinder-efficientnet-b0-starter-training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training efficientnet_b0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 準備"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import shutil\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from contextlib import contextmanager\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from functools import partial\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD\n",
    "import torchvision.models as models\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n",
    "\n",
    "import albumentations as transforms\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from albumentations import ImageOnlyTransform\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from pytorch_grad_cam import GradCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM\n",
    "\n",
    "sys.path.append('../input/pytorch-image-models/pytorch-image-models-master')\n",
    "import timm\n",
    "import lightgbm as lgb\n",
    "\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "OUTPUT_DIR = './my_model/'\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    apex=False\n",
    "    debug=False\n",
    "    print_freq=10\n",
    "    num_workers=4\n",
    "    size=512\n",
    "    model_name='tf_efficientnet_b0_ns'\n",
    "    scheduler='CosineAnnealingLR' # ['ReduceLROnPlateau', 'CosineAnnealingLR', 'CosineAnnealingWarmRestarts']\n",
    "    epochs=3\n",
    "    #factor=0.2 # ReduceLROnPlateau\n",
    "    #patience=4 # ReduceLROnPlateau\n",
    "    #eps=1e-6 # ReduceLROnPlateau\n",
    "    T_max=3 # CosineAnnealingLR\n",
    "    #T_0=3 # CosineAnnealingWarmRestarts\n",
    "    lr=1e-4\n",
    "    min_lr=1e-6\n",
    "    batch_size=16\n",
    "    weight_decay=1e-6\n",
    "    gradient_accumulation_steps=1\n",
    "    max_grad_norm=1000\n",
    "    seed=42\n",
    "    target_size=1\n",
    "    target_col='Pawpularity'\n",
    "    n_fold=2\n",
    "    trn_fold=[0, 1]\n",
    "    train=True\n",
    "    grad_cam=True\n",
    "    \n",
    "if CFG.debug:\n",
    "    CFG.epochs = 1\n",
    "    train = train.sample(n=1000, random_state=CFG.seed).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# wandb\n",
    "# ====================================================\n",
    "import wandb\n",
    "wandb.login\n",
    "\n",
    "def class2dict(f):\n",
    "    return dict((name, getattr(f, name)) for name in dir(f) if not name.startswith('__'))\n",
    "\n",
    "run = wandb.init(project=\"petfinder_myproject\", \n",
    "                 config=class2dict(CFG),\n",
    "                 job_type=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前処理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### deta load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Subject Focus</th>\n",
       "      <th>Eyes</th>\n",
       "      <th>Face</th>\n",
       "      <th>Near</th>\n",
       "      <th>Action</th>\n",
       "      <th>Accessory</th>\n",
       "      <th>Group</th>\n",
       "      <th>Collage</th>\n",
       "      <th>Human</th>\n",
       "      <th>Occlusion</th>\n",
       "      <th>Info</th>\n",
       "      <th>Blur</th>\n",
       "      <th>Pawpularity</th>\n",
       "      <th>file_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0007de18844b0dbbb5e1f607da0606e0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>../input/petfinder-pawpularity-score/train/000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0009c66b9439883ba2750fb825e1d7db</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>../input/petfinder-pawpularity-score/train/000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0013fd999caf9a3efe1352ca1b0d937e</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>../input/petfinder-pawpularity-score/train/001...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0018df346ac9c1d8413cfcc888ca8246</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>../input/petfinder-pawpularity-score/train/001...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>001dc955e10590d3ca4673f034feeef2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>../input/petfinder-pawpularity-score/train/001...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Id  Subject Focus  Eyes  Face  Near  Action  \\\n",
       "0  0007de18844b0dbbb5e1f607da0606e0              0     1     1     1       0   \n",
       "1  0009c66b9439883ba2750fb825e1d7db              0     1     1     0       0   \n",
       "2  0013fd999caf9a3efe1352ca1b0d937e              0     1     1     1       0   \n",
       "3  0018df346ac9c1d8413cfcc888ca8246              0     1     1     1       0   \n",
       "4  001dc955e10590d3ca4673f034feeef2              0     0     0     1       0   \n",
       "\n",
       "   Accessory  Group  Collage  Human  Occlusion  Info  Blur  Pawpularity  \\\n",
       "0          0      1        0      0          0     0     0           63   \n",
       "1          0      0        0      0          0     0     0           42   \n",
       "2          0      0        0      1          1     0     0           28   \n",
       "3          0      0        0      0          0     0     0           15   \n",
       "4          0      1        0      0          0     0     0           72   \n",
       "\n",
       "                                           file_path  \n",
       "0  ../input/petfinder-pawpularity-score/train/000...  \n",
       "1  ../input/petfinder-pawpularity-score/train/000...  \n",
       "2  ../input/petfinder-pawpularity-score/train/001...  \n",
       "3  ../input/petfinder-pawpularity-score/train/001...  \n",
       "4  ../input/petfinder-pawpularity-score/train/001...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Subject Focus</th>\n",
       "      <th>Eyes</th>\n",
       "      <th>Face</th>\n",
       "      <th>Near</th>\n",
       "      <th>Action</th>\n",
       "      <th>Accessory</th>\n",
       "      <th>Group</th>\n",
       "      <th>Collage</th>\n",
       "      <th>Human</th>\n",
       "      <th>Occlusion</th>\n",
       "      <th>Info</th>\n",
       "      <th>Blur</th>\n",
       "      <th>file_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4128bae22183829d2b5fea10effdb0c3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>../input/petfinder-pawpularity-score/test/4128...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>43a2262d7738e3d420d453815151079e</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>../input/petfinder-pawpularity-score/test/43a2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4e429cead1848a298432a0acad014c9d</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>../input/petfinder-pawpularity-score/test/4e42...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>80bc3ccafcc51b66303c2c263aa38486</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>../input/petfinder-pawpularity-score/test/80bc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8f49844c382931444e68dffbe20228f4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>../input/petfinder-pawpularity-score/test/8f49...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Id  Subject Focus  Eyes  Face  Near  Action  \\\n",
       "0  4128bae22183829d2b5fea10effdb0c3              1     0     1     0       0   \n",
       "1  43a2262d7738e3d420d453815151079e              0     1     0     0       0   \n",
       "2  4e429cead1848a298432a0acad014c9d              0     0     0     1       0   \n",
       "3  80bc3ccafcc51b66303c2c263aa38486              1     0     1     0       0   \n",
       "4  8f49844c382931444e68dffbe20228f4              1     1     1     0       1   \n",
       "\n",
       "   Accessory  Group  Collage  Human  Occlusion  Info  Blur  \\\n",
       "0          1      1        0      0          1     0     1   \n",
       "1          0      1        1      0          0     0     0   \n",
       "2          1      1        1      0          1     1     1   \n",
       "3          0      0        0      0          0     1     0   \n",
       "4          1      0        1      0          1     1     0   \n",
       "\n",
       "                                           file_path  \n",
       "0  ../input/petfinder-pawpularity-score/test/4128...  \n",
       "1  ../input/petfinder-pawpularity-score/test/43a2...  \n",
       "2  ../input/petfinder-pawpularity-score/test/4e42...  \n",
       "3  ../input/petfinder-pawpularity-score/test/80bc...  \n",
       "4  ../input/petfinder-pawpularity-score/test/8f49...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = pd.read_csv('../input/petfinder-pawpularity-score/train.csv')\n",
    "test = pd.read_csv('../input/petfinder-pawpularity-score/test.csv')\n",
    "\n",
    "def get_train_file_path(image_id):\n",
    "    return \"../input/petfinder-pawpularity-score/train/{}.jpg\".format(image_id)\n",
    "\n",
    "def get_test_file_path(image_id):\n",
    "    return \"../input/petfinder-pawpularity-score/test/{}.jpg\".format(image_id)\n",
    "\n",
    "train['file_path'] = train['Id'].apply(get_train_file_path)\n",
    "test['file_path'] = test['Id'].apply(get_test_file_path)\n",
    "\n",
    "display(train.head())\n",
    "display(test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = int(np.floor(1+np.log2(len(train))))\n",
    "train[\"bins\"] = pd.cut(train[CFG.target_col], bins=num_bins, labels=False)\n",
    "Fold = StratifiedKFold(n_splits=CFG.n_fold, shuffle=True, random_state=CFG.seed)\n",
    "for n, (train_index, val_index) in enumerate(Fold.split(train, train[\"bins\"])):\n",
    "    train.loc[val_index, 'fold'] = int(n)\n",
    "train['fold'] = train['fold'].astype(int)\n",
    "display(train.groupby(['fold', \"bins\"]).size())\n",
    "train.to_pickle(OUTPUT_DIR+'train.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seed値を固定\n",
    "def set_seed(seed =42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic =True\n",
    "set_seed(seed=CFG.seed)\n",
    "\n",
    "def get_transforms(*, data):\n",
    "    if data == 'train':\n",
    "        return transforms.Compose([\n",
    "            transforms.RandomResizedCrop(CFG.size, CFG.size, scale=(0.85, 1.0)),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225],\n",
    "            ),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "\n",
    "    elif data == 'valid':\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize(CFG.size, CFG.size),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225],\n",
    "            ),\n",
    "            ToTensorV2(),\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadDataset(Dataset):\n",
    "    def __init__(self,df,transform =None,isTrain = True):\n",
    "        self.df = df\n",
    "        self.file_names = df['file_path'].values\n",
    "        if(isTrain):\n",
    "            self.labels = df[CFG.target_col].values\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx,isTrain = True):\n",
    "        file_path = self.file_names[idx]\n",
    "        #cv2は画像読み込みなどに使われるライブラリ\n",
    "        image = cv2.imread(file_path)\n",
    "        #色空間を変換\n",
    "        #TODO 変換しない場合を実験\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        if self.transform:\n",
    "            image = self.transform(image=image)['image']\n",
    "        if(isTrain):\n",
    "            label = torch.tensor(self.labels[idx]).float()\n",
    "            return image, label\n",
    "        else:\n",
    "            return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = LoadDataset(train, transform=get_transforms(data='train'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, cfg, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.model = timm.create_model(self.cfg.model_name, pretrained=pretrained)\n",
    "        self.n_features = self.model.classifier.in_features\n",
    "        self.model.classifier = nn.Identity()\n",
    "        self.fc = nn.Linear(self.n_features, self.cfg.target_size)\n",
    "\n",
    "    def feature(self, image):\n",
    "        feature = self.model(image)\n",
    "        return feature\n",
    "        \n",
    "    def forward(self, image):\n",
    "        feature = self.feature(image)\n",
    "        output = self.fc(feature)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, yhat, y):\n",
    "        loss = torch.sqrt(self.mse(yhat, y) + self.eps)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def init_logger(log_file=OUTPUT_DIR+'train.log'):\n",
    "    from logging import getLogger, INFO, FileHandler,  Formatter,  StreamHandler\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=log_file)\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "LOGGER = init_logger()\n",
    "\n",
    "def get_scheduler(optimizer):\n",
    "        if CFG.scheduler=='ReduceLROnPlateau':\n",
    "            scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=CFG.factor, patience=CFG.patience, verbose=True, eps=CFG.eps)\n",
    "        elif CFG.scheduler=='CosineAnnealingLR':\n",
    "            scheduler = CosineAnnealingLR(optimizer, T_max=CFG.T_max, eta_min=CFG.min_lr, last_epoch=-1)\n",
    "        elif CFG.scheduler=='CosineAnnealingWarmRestarts':\n",
    "            scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=CFG.T_0, T_mult=1, eta_min=CFG.min_lr, last_epoch=-1)\n",
    "        return scheduler\n",
    "\n",
    "def get_RMSE(y_true,y_pred):\n",
    "    ## squared=FalseでRSCMになる。※TrueでMSE\n",
    "    score = mean_squared_error(y_true= y_true ,y_pred=y_pred,squared=False)\n",
    "    return score\n",
    "\n",
    "#RSMEを出力\n",
    "def get_result(result_df):\n",
    "    preds = result_df['preds'].values\n",
    "    labels = result_df[CFG.target_col].values\n",
    "    score = get_RMSE(labels, preds)\n",
    "    LOGGER.info(f'Score: {score:<.4f}')\n",
    "\n",
    "def train_fn(fold,train_loader,model,criterion,optimizer,epoch,scheduler,device):\n",
    "    model.train()\n",
    "    losses = AverageMeter()\n",
    "    global_step = 0\n",
    "    for step,(images,labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "        y_preds = model(images)\n",
    "        loss = criterion(y_preds.view(-1), labels)\n",
    "\n",
    "        # record loss\n",
    "        losses.update(loss.item(), batch_size)\n",
    "\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        loss.backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n",
    "        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "        if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n",
    "            print('Epoch: [{0}][{1}/{2}] '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  'Grad: {grad_norm:.4f}  '\n",
    "                  'LR: {lr:.6f}  '\n",
    "                  .format(epoch+1, step, len(train_loader),                         \n",
    "                          loss=losses,\n",
    "                          grad_norm=grad_norm,\n",
    "                          lr=scheduler.get_lr()[0]))\n",
    "        wandb.log({f\"[fold{fold}] loss\": losses.val,\n",
    "                   f\"[fold{fold}] lr\": scheduler.get_lr()[0]})\n",
    "\n",
    "def valid_fn(valid_loader, model, criterion, device):\n",
    "    #推論モードに切り替え\n",
    "    model.eval()\n",
    "    losses = AverageMeter()\n",
    "    preds = []\n",
    "    for step, (images, labels) in enumerate(valid_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "        # compute loss\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(images)\n",
    "        loss = criterion(y_preds.view(-1), labels)\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        # record accuracy\n",
    "        preds.append(y_preds.to('cpu').numpy())\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n",
    "            print('EVAL: [{0}/{1}] '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  .format(step, len(valid_loader),\n",
    "                          loss=losses,\n",
    "                          ))\n",
    "    predictions = np.concatenate(preds)\n",
    "    return losses.avg, predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(folds, fold):\n",
    "    LOGGER.info(f\"========== fold: {fold} training ==========\")\n",
    "\n",
    "    #dataset\n",
    "    trn_idx = folds[folds['fold'] != fold].index\n",
    "    val_idx = folds[folds['fold'] == fold].index\n",
    "    train_folds = folds.loc[trn_idx].reset_index(drop=True)\n",
    "    valid_folds = folds.loc[val_idx].reset_index(drop=True)\n",
    "    valid_labels = valid_folds[CFG.target_col].values\n",
    "    train_dataset = LoadDataset(train_folds, transform=get_transforms(data='train'))\n",
    "    valid_dataset = LoadDataset(valid_folds, transform=get_transforms(data='train'))\n",
    "\n",
    "    #dataloader\n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                              batch_size=CFG.batch_size, \n",
    "                              shuffle=True, \n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n",
    "    valid_loader = DataLoader(valid_dataset, \n",
    "                              batch_size=CFG.batch_size * 2, #TODO why\n",
    "                              shuffle=False, \n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "    \n",
    "    #model\n",
    "    model = Model(CFG, pretrained=True)\n",
    "    model.to(device)\n",
    "    optimizer = Adam(model.parameters(),lr= CFG.lr,weight_decay=CFG.weight_decay,amsgrad=False)\n",
    "    scheduler = get_scheduler(optimizer)\n",
    "    criterion = RMSELoss()\n",
    "\n",
    "    #train loop\n",
    "    best_score = np.inf\n",
    "    best_loss = np.inf\n",
    "    for epoch in range(CFG.epochs):\n",
    "        # train\n",
    "        avg_loss = train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device)\n",
    "        \n",
    "        # validation\n",
    "        avg_val_loss, preds = valid_fn(valid_loader, model, criterion, device)\n",
    "\n",
    "        if isinstance(scheduler, ReduceLROnPlateau):\n",
    "            scheduler.step(avg_val_loss)\n",
    "        elif isinstance(scheduler, CosineAnnealingLR):\n",
    "            scheduler.step()\n",
    "        elif isinstance(scheduler, CosineAnnealingWarmRestarts):\n",
    "            scheduler.step()\n",
    "\n",
    "        # scoring\n",
    "        score = get_RMSE(valid_labels, preds)\n",
    "        LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}')\n",
    "        wandb.log({f\"[fold{fold}] epoch\": epoch+1, \n",
    "                   f\"[fold{fold}] avg_train_loss\": avg_loss, \n",
    "                   f\"[fold{fold}] avg_val_loss\": avg_val_loss,\n",
    "                   f\"[fold{fold}] score\": score})\n",
    "        if score < best_score:\n",
    "            best_score = score\n",
    "            LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
    "            torch.save({'model': model.state_dict(), \n",
    "                        'preds': preds},\n",
    "                        OUTPUT_DIR+f'{CFG.model_name}_fold{fold}_best.pth')\n",
    "        valid_folds['preds'] = torch.load(OUTPUT_DIR+f'{CFG.model_name}_fold{fold}_best.pth', \n",
    "                                      map_location=torch.device('cpu'))['preds']\n",
    "\n",
    "    return valid_folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    if CFG.train:\n",
    "        # train \n",
    "        oof_df = pd.DataFrame()\n",
    "        for fold in range(CFG.n_fold):\n",
    "            if fold in CFG.trn_fold:\n",
    "                _oof_df = train_loop(train, fold)\n",
    "                oof_df = pd.concat([oof_df, _oof_df])\n",
    "                LOGGER.info(f\"========== fold: {fold} result ==========\")\n",
    "                get_result(_oof_df)\n",
    "        LOGGER.info(f\"========== CV ==========\")\n",
    "        get_result(oof_df)\n",
    "        #結果を保存\n",
    "        oof_df.to_csv(OUTPUT_DIR+'oof_df.csv', index=False)\n",
    "    wandb.finish()\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGB [training]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 設定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### directory設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "OUTPUT_DIR = './my_model'\n",
    "#TODO\n",
    "MODEL_DIR = '../my_model/'\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CFG:\n",
    "    num_workers=4\n",
    "    size=512\n",
    "    batch_size=32\n",
    "    model_name='tf_efficientnet_b0_ns'\n",
    "    seed=42\n",
    "    target_size=1\n",
    "    target_col='Pawpularity'\n",
    "    n_fold=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fold  bins\n",
       "0     0        165\n",
       "      1        209\n",
       "      2        551\n",
       "      3       1014\n",
       "      4        941\n",
       "      5        650\n",
       "      6        420\n",
       "      7        267\n",
       "      8        203\n",
       "      9        137\n",
       "      10        99\n",
       "      11        70\n",
       "      12        51\n",
       "      13       179\n",
       "1     0        165\n",
       "      1        209\n",
       "      2        550\n",
       "      3       1015\n",
       "      4        942\n",
       "      5        649\n",
       "      6        419\n",
       "      7        266\n",
       "      8        203\n",
       "      9        137\n",
       "      10        99\n",
       "      11        70\n",
       "      12        52\n",
       "      13       180\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = pd.read_pickle('../my_model/train.pkl')\n",
    "\n",
    "display(train.groupby(['fold', \"bins\"]).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(test_loader, model, device):\n",
    "    model.eval()\n",
    "    features = []\n",
    "    tk0 = tqdm(enumerate(test_loader), total=len(test_loader))\n",
    "    for step, (images) in tk0:\n",
    "        images = images.to(device)\n",
    "        batch_size = images.size(0)\n",
    "        with torch.no_grad():\n",
    "            feature = model.feature(images)\n",
    "        features.append(feature.to('cpu').numpy())\n",
    "    features = np.concatenate(features)\n",
    "    return features\n",
    "\n",
    "def run_single_lightgbm(param, train, features, target, fold=0, categorical=[]):\n",
    "    #dataload\n",
    "    train[[f\"img_{i}\" for i in np.arange(1280)]] = IMG_FEATURES[fold]\n",
    "    trn_idx = train[train.fold != fold].index\n",
    "    val_idx = train[train.fold == fold].index\n",
    "    LOGGER.info(f'train size : {len(trn_idx)}  valid size : {len(val_idx)}')\n",
    "    if categorical == []:\n",
    "        trn_data = lgb.Dataset(train.iloc[trn_idx][features].values, label=target.iloc[trn_idx].values)\n",
    "        val_data = lgb.Dataset(train.iloc[val_idx][features].values, label=target.iloc[val_idx].values)\n",
    "    else:\n",
    "        trn_data = lgb.Dataset(train.iloc[trn_idx][features], label=target.iloc[trn_idx].values, categorical_feature=categorical)\n",
    "        val_data = lgb.Dataset(train.iloc[val_idx][features], label=target.iloc[val_idx].values, categorical_feature=categorical)\n",
    "    num_round = 10000\n",
    "\n",
    "    #train\n",
    "    clf = lgb.train(param, \n",
    "                    trn_data,\n",
    "                    num_round,\n",
    "                    valid_sets=[trn_data, val_data],\n",
    "                    verbose_eval=10,\n",
    "                    early_stopping_rounds=10)\n",
    "    LOGGER.info(f'Dumping model with pickle... lightgbm_fold{fold}.pkl')\n",
    "    with open(OUTPUT_DIR+f'lightgbm_fold{fold}.pkl', 'wb') as fout:\n",
    "        #モデルをシリアライズ化して保存\n",
    "        pickle.dump(clf, fout)\n",
    "    \n",
    "    #予測\n",
    "    oof = np.zeros(len(train))\n",
    "    oof[val_idx] = clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n",
    "    score = get_RMSE(target.iloc[val_idx].values, oof[val_idx])\n",
    "    LOGGER.info(f\"fold{fold} score: {score:<.5f}\")\n",
    "    \n",
    "    #future_importance\n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"Feature\"] = features\n",
    "    fold_importance_df[\"importance\"] = clf.feature_importance(importance_type='gain')\n",
    "    fold_importance_df[\"fold\"] = fold\n",
    "\n",
    "    return oof, fold_importance_df, val_idx\n",
    "\n",
    "\n",
    "def run_kfold_lightgbm(param, train, features, target, n_fold=5, categorical=[]):\n",
    "    oof = np.zeros(len(train))\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    val_idxes = []\n",
    "    \n",
    "    for fold in range(n_fold):\n",
    "        LOGGER.info(f\"===== Fold {fold} =====\")\n",
    "        _oof, fold_importance_df, val_idx = run_single_lightgbm(param, \n",
    "                                                                train, features, target, \n",
    "                                                                fold=fold, categorical=categorical)\n",
    "        oof += _oof\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "        val_idxes.append(val_idx)\n",
    "    \n",
    "    val_idxes = np.concatenate(val_idxes)\n",
    "    score = get_RMSE(target.iloc[val_idxes].values, oof[val_idxes])\n",
    "    LOGGER.info(f\"CV score: {score:<.5f}\")\n",
    "    \n",
    "    return oof, feature_importance_df, val_idxes\n",
    "\n",
    "\n",
    "def show_feature_importance(feature_importance_df):\n",
    "    cols = (feature_importance_df[[\"Feature\", \"importance\"]]\n",
    "                .groupby(\"Feature\").mean().sort_values(by=\"importance\", ascending=False)[:50].index)\n",
    "    best_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n",
    "    plt.figure(figsize=(8, 16))\n",
    "    sns.barplot(x=\"importance\", y=\"Feature\", data=best_features.sort_values(by=\"importance\",ascending=False))\n",
    "    plt.title('Features importance (averaged/folds)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUTPUT_DIR+'feature_importance_df_lightgbm.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23f5a085ddad4f8481413b1e94ba22c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/155 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/155 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c00e577a6ce64d1096233249587c5b9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/155 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/155 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "IMG_FEATURES = []\n",
    "test_dataset = LoadDataset(train, transform=get_transforms(data='valid'),isTrain=False)\n",
    "test_loader = DataLoader(test_dataset, \n",
    "                         batch_size=CFG.batch_size * 2, \n",
    "                         shuffle=False, \n",
    "                         num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "for fold in range(CFG.n_fold):\n",
    "    model = Model(CFG, pretrained=False)\n",
    "    state = torch.load(MODEL_DIR+f'{CFG.model_name}_fold{fold}_best.pth', \n",
    "                       map_location=torch.device('cpu'))['model']\n",
    "    model.load_state_dict(state)\n",
    "    model.to(device)\n",
    "    features = get_features(test_loader, model, device)\n",
    "    IMG_FEATURES.append(features)\n",
    "    del state; gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "===== Fold 0 =====\n",
      "train size : 4956  valid size : 4956\n",
      "Dumping model with pickle... lightgbm_fold0.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's rmse: 20.5749\tvalid_1's rmse: 20.5689\n",
      "[20]\ttraining's rmse: 20.5494\tvalid_1's rmse: 20.5641\n",
      "[30]\ttraining's rmse: 20.5276\tvalid_1's rmse: 20.5618\n",
      "[40]\ttraining's rmse: 20.51\tvalid_1's rmse: 20.5615\n",
      "Early stopping, best iteration is:\n",
      "[36]\ttraining's rmse: 20.5166\tvalid_1's rmse: 20.5614\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "DataFrame.dtypes for data must be int, float or bool.\nDid not expect the data types in the following fields: img_0, img_1, img_2, img_3, img_4, img_5, img_6, img_7, img_8, img_9, img_10, img_11, img_12, img_13, img_14, img_15, img_16, img_17, img_18, img_19, img_20, img_21, img_22, img_23, img_24, img_25, img_26, img_27, img_28, img_29, img_30, img_31, img_32, img_33, img_34, img_35, img_36, img_37, img_38, img_39, img_40, img_41, img_42, img_43, img_44, img_45, img_46, img_47, img_48, img_49, img_50, img_51, img_52, img_53, img_54, img_55, img_56, img_57, img_58, img_59, img_60, img_61, img_62, img_63, img_64, img_65, img_66, img_67, img_68, img_69, img_70, img_71, img_72, img_73, img_74, img_75, img_76, img_77, img_78, img_79, img_80, img_81, img_82, img_83, img_84, img_85, img_86, img_87, img_88, img_89, img_90, img_91, img_92, img_93, img_94, img_95, img_96, img_97, img_98, img_99, img_100, img_101, img_102, img_103, img_104, img_105, img_106, img_107, img_108, img_109, img_110, img_111, img_112, img_113, img_114, img_115, img_116, img_117, img_118, img_119, img_120, img_121, img_122, img_123, img_124, img_125, img_126, img_127, img_128, img_129, img_130, img_131, img_132, img_133, img_134, img_135, img_136, img_137, img_138, img_139, img_140, img_141, img_142, img_143, img_144, img_145, img_146, img_147, img_148, img_149, img_150, img_151, img_152, img_153, img_154, img_155, img_156, img_157, img_158, img_159, img_160, img_161, img_162, img_163, img_164, img_165, img_166, img_167, img_168, img_169, img_170, img_171, img_172, img_173, img_174, img_175, img_176, img_177, img_178, img_179, img_180, img_181, img_182, img_183, img_184, img_185, img_186, img_187, img_188, img_189, img_190, img_191, img_192, img_193, img_194, img_195, img_196, img_197, img_198, img_199, img_200, img_201, img_202, img_203, img_204, img_205, img_206, img_207, img_208, img_209, img_210, img_211, img_212, img_213, img_214, img_215, img_216, img_217, img_218, img_219, img_220, img_221, img_222, img_223, img_224, img_225, img_226, img_227, img_228, img_229, img_230, img_231, img_232, img_233, img_234, img_235, img_236, img_237, img_238, img_239, img_240, img_241, img_242, img_243, img_244, img_245, img_246, img_247, img_248, img_249, img_250, img_251, img_252, img_253, img_254, img_255, img_256, img_257, img_258, img_259, img_260, img_261, img_262, img_263, img_264, img_265, img_266, img_267, img_268, img_269, img_270, img_271, img_272, img_273, img_274, img_275, img_276, img_277, img_278, img_279, img_280, img_281, img_282, img_283, img_284, img_285, img_286, img_287, img_288, img_289, img_290, img_291, img_292, img_293, img_294, img_295, img_296, img_297, img_298, img_299, img_300, img_301, img_302, img_303, img_304, img_305, img_306, img_307, img_308, img_309, img_310, img_311, img_312, img_313, img_314, img_315, img_316, img_317, img_318, img_319, img_320, img_321, img_322, img_323, img_324, img_325, img_326, img_327, img_328, img_329, img_330, img_331, img_332, img_333, img_334, img_335, img_336, img_337, img_338, img_339, img_340, img_341, img_342, img_343, img_344, img_345, img_346, img_347, img_348, img_349, img_350, img_351, img_352, img_353, img_354, img_355, img_356, img_357, img_358, img_359, img_360, img_361, img_362, img_363, img_364, img_365, img_366, img_367, img_368, img_369, img_370, img_371, img_372, img_373, img_374, img_375, img_376, img_377, img_378, img_379, img_380, img_381, img_382, img_383, img_384, img_385, img_386, img_387, img_388, img_389, img_390, img_391, img_392, img_393, img_394, img_395, img_396, img_397, img_398, img_399, img_400, img_401, img_402, img_403, img_404, img_405, img_406, img_407, img_408, img_409, img_410, img_411, img_412, img_413, img_414, img_415, img_416, img_417, img_418, img_419, img_420, img_421, img_422, img_423, img_424, img_425, img_426, img_427, img_428, img_429, img_430, img_431, img_432, img_433, img_434, img_435, img_436, img_437, img_438, img_439, img_440, img_441, img_442, img_443, img_444, img_445, img_446, img_447, img_448, img_449, img_450, img_451, img_452, img_453, img_454, img_455, img_456, img_457, img_458, img_459, img_460, img_461, img_462, img_463, img_464, img_465, img_466, img_467, img_468, img_469, img_470, img_471, img_472, img_473, img_474, img_475, img_476, img_477, img_478, img_479, img_480, img_481, img_482, img_483, img_484, img_485, img_486, img_487, img_488, img_489, img_490, img_491, img_492, img_493, img_494, img_495, img_496, img_497, img_498, img_499, img_500, img_501, img_502, img_503, img_504, img_505, img_506, img_507, img_508, img_509, img_510, img_511, img_512, img_513, img_514, img_515, img_516, img_517, img_518, img_519, img_520, img_521, img_522, img_523, img_524, img_525, img_526, img_527, img_528, img_529, img_530, img_531, img_532, img_533, img_534, img_535, img_536, img_537, img_538, img_539, img_540, img_541, img_542, img_543, img_544, img_545, img_546, img_547, img_548, img_549, img_550, img_551, img_552, img_553, img_554, img_555, img_556, img_557, img_558, img_559, img_560, img_561, img_562, img_563, img_564, img_565, img_566, img_567, img_568, img_569, img_570, img_571, img_572, img_573, img_574, img_575, img_576, img_577, img_578, img_579, img_580, img_581, img_582, img_583, img_584, img_585, img_586, img_587, img_588, img_589, img_590, img_591, img_592, img_593, img_594, img_595, img_596, img_597, img_598, img_599, img_600, img_601, img_602, img_603, img_604, img_605, img_606, img_607, img_608, img_609, img_610, img_611, img_612, img_613, img_614, img_615, img_616, img_617, img_618, img_619, img_620, img_621, img_622, img_623, img_624, img_625, img_626, img_627, img_628, img_629, img_630, img_631, img_632, img_633, img_634, img_635, img_636, img_637, img_638, img_639, img_640, img_641, img_642, img_643, img_644, img_645, img_646, img_647, img_648, img_649, img_650, img_651, img_652, img_653, img_654, img_655, img_656, img_657, img_658, img_659, img_660, img_661, img_662, img_663, img_664, img_665, img_666, img_667, img_668, img_669, img_670, img_671, img_672, img_673, img_674, img_675, img_676, img_677, img_678, img_679, img_680, img_681, img_682, img_683, img_684, img_685, img_686, img_687, img_688, img_689, img_690, img_691, img_692, img_693, img_694, img_695, img_696, img_697, img_698, img_699, img_700, img_701, img_702, img_703, img_704, img_705, img_706, img_707, img_708, img_709, img_710, img_711, img_712, img_713, img_714, img_715, img_716, img_717, img_718, img_719, img_720, img_721, img_722, img_723, img_724, img_725, img_726, img_727, img_728, img_729, img_730, img_731, img_732, img_733, img_734, img_735, img_736, img_737, img_738, img_739, img_740, img_741, img_742, img_743, img_744, img_745, img_746, img_747, img_748, img_749, img_750, img_751, img_752, img_753, img_754, img_755, img_756, img_757, img_758, img_759, img_760, img_761, img_762, img_763, img_764, img_765, img_766, img_767, img_768, img_769, img_770, img_771, img_772, img_773, img_774, img_775, img_776, img_777, img_778, img_779, img_780, img_781, img_782, img_783, img_784, img_785, img_786, img_787, img_788, img_789, img_790, img_791, img_792, img_793, img_794, img_795, img_796, img_797, img_798, img_799, img_800, img_801, img_802, img_803, img_804, img_805, img_806, img_807, img_808, img_809, img_810, img_811, img_812, img_813, img_814, img_815, img_816, img_817, img_818, img_819, img_820, img_821, img_822, img_823, img_824, img_825, img_826, img_827, img_828, img_829, img_830, img_831, img_832, img_833, img_834, img_835, img_836, img_837, img_838, img_839, img_840, img_841, img_842, img_843, img_844, img_845, img_846, img_847, img_848, img_849, img_850, img_851, img_852, img_853, img_854, img_855, img_856, img_857, img_858, img_859, img_860, img_861, img_862, img_863, img_864, img_865, img_866, img_867, img_868, img_869, img_870, img_871, img_872, img_873, img_874, img_875, img_876, img_877, img_878, img_879, img_880, img_881, img_882, img_883, img_884, img_885, img_886, img_887, img_888, img_889, img_890, img_891, img_892, img_893, img_894, img_895, img_896, img_897, img_898, img_899, img_900, img_901, img_902, img_903, img_904, img_905, img_906, img_907, img_908, img_909, img_910, img_911, img_912, img_913, img_914, img_915, img_916, img_917, img_918, img_919, img_920, img_921, img_922, img_923, img_924, img_925, img_926, img_927, img_928, img_929, img_930, img_931, img_932, img_933, img_934, img_935, img_936, img_937, img_938, img_939, img_940, img_941, img_942, img_943, img_944, img_945, img_946, img_947, img_948, img_949, img_950, img_951, img_952, img_953, img_954, img_955, img_956, img_957, img_958, img_959, img_960, img_961, img_962, img_963, img_964, img_965, img_966, img_967, img_968, img_969, img_970, img_971, img_972, img_973, img_974, img_975, img_976, img_977, img_978, img_979, img_980, img_981, img_982, img_983, img_984, img_985, img_986, img_987, img_988, img_989, img_990, img_991, img_992, img_993, img_994, img_995, img_996, img_997, img_998, img_999, img_1000, img_1001, img_1002, img_1003, img_1004, img_1005, img_1006, img_1007, img_1008, img_1009, img_1010, img_1011, img_1012, img_1013, img_1014, img_1015, img_1016, img_1017, img_1018, img_1019, img_1020, img_1021, img_1022, img_1023, img_1024, img_1025, img_1026, img_1027, img_1028, img_1029, img_1030, img_1031, img_1032, img_1033, img_1034, img_1035, img_1036, img_1037, img_1038, img_1039, img_1040, img_1041, img_1042, img_1043, img_1044, img_1045, img_1046, img_1047, img_1048, img_1049, img_1050, img_1051, img_1052, img_1053, img_1054, img_1055, img_1056, img_1057, img_1058, img_1059, img_1060, img_1061, img_1062, img_1063, img_1064, img_1065, img_1066, img_1067, img_1068, img_1069, img_1070, img_1071, img_1072, img_1073, img_1074, img_1075, img_1076, img_1077, img_1078, img_1079, img_1080, img_1081, img_1082, img_1083, img_1084, img_1085, img_1086, img_1087, img_1088, img_1089, img_1090, img_1091, img_1092, img_1093, img_1094, img_1095, img_1096, img_1097, img_1098, img_1099, img_1100, img_1101, img_1102, img_1103, img_1104, img_1105, img_1106, img_1107, img_1108, img_1109, img_1110, img_1111, img_1112, img_1113, img_1114, img_1115, img_1116, img_1117, img_1118, img_1119, img_1120, img_1121, img_1122, img_1123, img_1124, img_1125, img_1126, img_1127, img_1128, img_1129, img_1130, img_1131, img_1132, img_1133, img_1134, img_1135, img_1136, img_1137, img_1138, img_1139, img_1140, img_1141, img_1142, img_1143, img_1144, img_1145, img_1146, img_1147, img_1148, img_1149, img_1150, img_1151, img_1152, img_1153, img_1154, img_1155, img_1156, img_1157, img_1158, img_1159, img_1160, img_1161, img_1162, img_1163, img_1164, img_1165, img_1166, img_1167, img_1168, img_1169, img_1170, img_1171, img_1172, img_1173, img_1174, img_1175, img_1176, img_1177, img_1178, img_1179, img_1180, img_1181, img_1182, img_1183, img_1184, img_1185, img_1186, img_1187, img_1188, img_1189, img_1190, img_1191, img_1192, img_1193, img_1194, img_1195, img_1196, img_1197, img_1198, img_1199, img_1200, img_1201, img_1202, img_1203, img_1204, img_1205, img_1206, img_1207, img_1208, img_1209, img_1210, img_1211, img_1212, img_1213, img_1214, img_1215, img_1216, img_1217, img_1218, img_1219, img_1220, img_1221, img_1222, img_1223, img_1224, img_1225, img_1226, img_1227, img_1228, img_1229, img_1230, img_1231, img_1232, img_1233, img_1234, img_1235, img_1236, img_1237, img_1238, img_1239, img_1240, img_1241, img_1242, img_1243, img_1244, img_1245, img_1246, img_1247, img_1248, img_1249, img_1250, img_1251, img_1252, img_1253, img_1254, img_1255, img_1256, img_1257, img_1258, img_1259, img_1260, img_1261, img_1262, img_1263, img_1264, img_1265, img_1266, img_1267, img_1268, img_1269, img_1270, img_1271, img_1272, img_1273, img_1274, img_1275, img_1276, img_1277, img_1278, img_1279",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12123/1278416095.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m oof, feature_importance_df, _ = run_kfold_lightgbm(lgb_param, \n\u001b[1;32m     17\u001b[0m                                                    \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                                                    n_fold=5, categorical=[])\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_12123/4254781603.py\u001b[0m in \u001b[0;36mrun_kfold_lightgbm\u001b[0;34m(param, train, features, target, n_fold, categorical)\u001b[0m\n\u001b[1;32m     63\u001b[0m         _oof, fold_importance_df, val_idx = run_single_lightgbm(param, \n\u001b[1;32m     64\u001b[0m                                                                 \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                                                                 fold=fold, categorical=categorical)\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0moof\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0m_oof\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mfeature_importance_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_importance_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold_importance_df\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_12123/4254781603.py\u001b[0m in \u001b[0;36mrun_single_lightgbm\u001b[0;34m(param, train, features, target, fold, categorical)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m#予測\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0moof\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0moof\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_iteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_RMSE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moof\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"fold{fold} score: {score:<.5f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, start_iteration, num_iteration, raw_score, pred_leaf, pred_contrib, data_has_header, is_reshape, **kwargs)\u001b[0m\n\u001b[1;32m   3538\u001b[0m         return predictor.predict(data, start_iteration, num_iteration,\n\u001b[1;32m   3539\u001b[0m                                  \u001b[0mraw_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_leaf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_contrib\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3540\u001b[0;31m                                  data_has_header, is_reshape)\n\u001b[0m\u001b[1;32m   3541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3542\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrefit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, start_iteration, num_iteration, raw_score, pred_leaf, pred_contrib, data_has_header, is_reshape)\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot use Dataset instance for prediction, please use raw data instead\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 820\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_data_from_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpandas_categorical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    821\u001b[0m         \u001b[0mpredict_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mC_API_PREDICT_NORMAL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mraw_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m_data_from_pandas\u001b[0;34m(data, feature_name, categorical_feature, pandas_categorical)\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbad_indices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m             \u001b[0mbad_index_cols_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbad_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m             raise ValueError(\"DataFrame.dtypes for data must be int, float or bool.\\n\"\n\u001b[0m\u001b[1;32m    595\u001b[0m                              \u001b[0;34m\"Did not expect the data types in the following fields: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m                              f\"{bad_index_cols_str}\")\n",
      "\u001b[0;31mValueError\u001b[0m: DataFrame.dtypes for data must be int, float or bool.\nDid not expect the data types in the following fields: img_0, img_1, img_2, img_3, img_4, img_5, img_6, img_7, img_8, img_9, img_10, img_11, img_12, img_13, img_14, img_15, img_16, img_17, img_18, img_19, img_20, img_21, img_22, img_23, img_24, img_25, img_26, img_27, img_28, img_29, img_30, img_31, img_32, img_33, img_34, img_35, img_36, img_37, img_38, img_39, img_40, img_41, img_42, img_43, img_44, img_45, img_46, img_47, img_48, img_49, img_50, img_51, img_52, img_53, img_54, img_55, img_56, img_57, img_58, img_59, img_60, img_61, img_62, img_63, img_64, img_65, img_66, img_67, img_68, img_69, img_70, img_71, img_72, img_73, img_74, img_75, img_76, img_77, img_78, img_79, img_80, img_81, img_82, img_83, img_84, img_85, img_86, img_87, img_88, img_89, img_90, img_91, img_92, img_93, img_94, img_95, img_96, img_97, img_98, img_99, img_100, img_101, img_102, img_103, img_104, img_105, img_106, img_107, img_108, img_109, img_110, img_111, img_112, img_113, img_114, img_115, img_116, img_117, img_118, img_119, img_120, img_121, img_122, img_123, img_124, img_125, img_126, img_127, img_128, img_129, img_130, img_131, img_132, img_133, img_134, img_135, img_136, img_137, img_138, img_139, img_140, img_141, img_142, img_143, img_144, img_145, img_146, img_147, img_148, img_149, img_150, img_151, img_152, img_153, img_154, img_155, img_156, img_157, img_158, img_159, img_160, img_161, img_162, img_163, img_164, img_165, img_166, img_167, img_168, img_169, img_170, img_171, img_172, img_173, img_174, img_175, img_176, img_177, img_178, img_179, img_180, img_181, img_182, img_183, img_184, img_185, img_186, img_187, img_188, img_189, img_190, img_191, img_192, img_193, img_194, img_195, img_196, img_197, img_198, img_199, img_200, img_201, img_202, img_203, img_204, img_205, img_206, img_207, img_208, img_209, img_210, img_211, img_212, img_213, img_214, img_215, img_216, img_217, img_218, img_219, img_220, img_221, img_222, img_223, img_224, img_225, img_226, img_227, img_228, img_229, img_230, img_231, img_232, img_233, img_234, img_235, img_236, img_237, img_238, img_239, img_240, img_241, img_242, img_243, img_244, img_245, img_246, img_247, img_248, img_249, img_250, img_251, img_252, img_253, img_254, img_255, img_256, img_257, img_258, img_259, img_260, img_261, img_262, img_263, img_264, img_265, img_266, img_267, img_268, img_269, img_270, img_271, img_272, img_273, img_274, img_275, img_276, img_277, img_278, img_279, img_280, img_281, img_282, img_283, img_284, img_285, img_286, img_287, img_288, img_289, img_290, img_291, img_292, img_293, img_294, img_295, img_296, img_297, img_298, img_299, img_300, img_301, img_302, img_303, img_304, img_305, img_306, img_307, img_308, img_309, img_310, img_311, img_312, img_313, img_314, img_315, img_316, img_317, img_318, img_319, img_320, img_321, img_322, img_323, img_324, img_325, img_326, img_327, img_328, img_329, img_330, img_331, img_332, img_333, img_334, img_335, img_336, img_337, img_338, img_339, img_340, img_341, img_342, img_343, img_344, img_345, img_346, img_347, img_348, img_349, img_350, img_351, img_352, img_353, img_354, img_355, img_356, img_357, img_358, img_359, img_360, img_361, img_362, img_363, img_364, img_365, img_366, img_367, img_368, img_369, img_370, img_371, img_372, img_373, img_374, img_375, img_376, img_377, img_378, img_379, img_380, img_381, img_382, img_383, img_384, img_385, img_386, img_387, img_388, img_389, img_390, img_391, img_392, img_393, img_394, img_395, img_396, img_397, img_398, img_399, img_400, img_401, img_402, img_403, img_404, img_405, img_406, img_407, img_408, img_409, img_410, img_411, img_412, img_413, img_414, img_415, img_416, img_417, img_418, img_419, img_420, img_421, img_422, img_423, img_424, img_425, img_426, img_427, img_428, img_429, img_430, img_431, img_432, img_433, img_434, img_435, img_436, img_437, img_438, img_439, img_440, img_441, img_442, img_443, img_444, img_445, img_446, img_447, img_448, img_449, img_450, img_451, img_452, img_453, img_454, img_455, img_456, img_457, img_458, img_459, img_460, img_461, img_462, img_463, img_464, img_465, img_466, img_467, img_468, img_469, img_470, img_471, img_472, img_473, img_474, img_475, img_476, img_477, img_478, img_479, img_480, img_481, img_482, img_483, img_484, img_485, img_486, img_487, img_488, img_489, img_490, img_491, img_492, img_493, img_494, img_495, img_496, img_497, img_498, img_499, img_500, img_501, img_502, img_503, img_504, img_505, img_506, img_507, img_508, img_509, img_510, img_511, img_512, img_513, img_514, img_515, img_516, img_517, img_518, img_519, img_520, img_521, img_522, img_523, img_524, img_525, img_526, img_527, img_528, img_529, img_530, img_531, img_532, img_533, img_534, img_535, img_536, img_537, img_538, img_539, img_540, img_541, img_542, img_543, img_544, img_545, img_546, img_547, img_548, img_549, img_550, img_551, img_552, img_553, img_554, img_555, img_556, img_557, img_558, img_559, img_560, img_561, img_562, img_563, img_564, img_565, img_566, img_567, img_568, img_569, img_570, img_571, img_572, img_573, img_574, img_575, img_576, img_577, img_578, img_579, img_580, img_581, img_582, img_583, img_584, img_585, img_586, img_587, img_588, img_589, img_590, img_591, img_592, img_593, img_594, img_595, img_596, img_597, img_598, img_599, img_600, img_601, img_602, img_603, img_604, img_605, img_606, img_607, img_608, img_609, img_610, img_611, img_612, img_613, img_614, img_615, img_616, img_617, img_618, img_619, img_620, img_621, img_622, img_623, img_624, img_625, img_626, img_627, img_628, img_629, img_630, img_631, img_632, img_633, img_634, img_635, img_636, img_637, img_638, img_639, img_640, img_641, img_642, img_643, img_644, img_645, img_646, img_647, img_648, img_649, img_650, img_651, img_652, img_653, img_654, img_655, img_656, img_657, img_658, img_659, img_660, img_661, img_662, img_663, img_664, img_665, img_666, img_667, img_668, img_669, img_670, img_671, img_672, img_673, img_674, img_675, img_676, img_677, img_678, img_679, img_680, img_681, img_682, img_683, img_684, img_685, img_686, img_687, img_688, img_689, img_690, img_691, img_692, img_693, img_694, img_695, img_696, img_697, img_698, img_699, img_700, img_701, img_702, img_703, img_704, img_705, img_706, img_707, img_708, img_709, img_710, img_711, img_712, img_713, img_714, img_715, img_716, img_717, img_718, img_719, img_720, img_721, img_722, img_723, img_724, img_725, img_726, img_727, img_728, img_729, img_730, img_731, img_732, img_733, img_734, img_735, img_736, img_737, img_738, img_739, img_740, img_741, img_742, img_743, img_744, img_745, img_746, img_747, img_748, img_749, img_750, img_751, img_752, img_753, img_754, img_755, img_756, img_757, img_758, img_759, img_760, img_761, img_762, img_763, img_764, img_765, img_766, img_767, img_768, img_769, img_770, img_771, img_772, img_773, img_774, img_775, img_776, img_777, img_778, img_779, img_780, img_781, img_782, img_783, img_784, img_785, img_786, img_787, img_788, img_789, img_790, img_791, img_792, img_793, img_794, img_795, img_796, img_797, img_798, img_799, img_800, img_801, img_802, img_803, img_804, img_805, img_806, img_807, img_808, img_809, img_810, img_811, img_812, img_813, img_814, img_815, img_816, img_817, img_818, img_819, img_820, img_821, img_822, img_823, img_824, img_825, img_826, img_827, img_828, img_829, img_830, img_831, img_832, img_833, img_834, img_835, img_836, img_837, img_838, img_839, img_840, img_841, img_842, img_843, img_844, img_845, img_846, img_847, img_848, img_849, img_850, img_851, img_852, img_853, img_854, img_855, img_856, img_857, img_858, img_859, img_860, img_861, img_862, img_863, img_864, img_865, img_866, img_867, img_868, img_869, img_870, img_871, img_872, img_873, img_874, img_875, img_876, img_877, img_878, img_879, img_880, img_881, img_882, img_883, img_884, img_885, img_886, img_887, img_888, img_889, img_890, img_891, img_892, img_893, img_894, img_895, img_896, img_897, img_898, img_899, img_900, img_901, img_902, img_903, img_904, img_905, img_906, img_907, img_908, img_909, img_910, img_911, img_912, img_913, img_914, img_915, img_916, img_917, img_918, img_919, img_920, img_921, img_922, img_923, img_924, img_925, img_926, img_927, img_928, img_929, img_930, img_931, img_932, img_933, img_934, img_935, img_936, img_937, img_938, img_939, img_940, img_941, img_942, img_943, img_944, img_945, img_946, img_947, img_948, img_949, img_950, img_951, img_952, img_953, img_954, img_955, img_956, img_957, img_958, img_959, img_960, img_961, img_962, img_963, img_964, img_965, img_966, img_967, img_968, img_969, img_970, img_971, img_972, img_973, img_974, img_975, img_976, img_977, img_978, img_979, img_980, img_981, img_982, img_983, img_984, img_985, img_986, img_987, img_988, img_989, img_990, img_991, img_992, img_993, img_994, img_995, img_996, img_997, img_998, img_999, img_1000, img_1001, img_1002, img_1003, img_1004, img_1005, img_1006, img_1007, img_1008, img_1009, img_1010, img_1011, img_1012, img_1013, img_1014, img_1015, img_1016, img_1017, img_1018, img_1019, img_1020, img_1021, img_1022, img_1023, img_1024, img_1025, img_1026, img_1027, img_1028, img_1029, img_1030, img_1031, img_1032, img_1033, img_1034, img_1035, img_1036, img_1037, img_1038, img_1039, img_1040, img_1041, img_1042, img_1043, img_1044, img_1045, img_1046, img_1047, img_1048, img_1049, img_1050, img_1051, img_1052, img_1053, img_1054, img_1055, img_1056, img_1057, img_1058, img_1059, img_1060, img_1061, img_1062, img_1063, img_1064, img_1065, img_1066, img_1067, img_1068, img_1069, img_1070, img_1071, img_1072, img_1073, img_1074, img_1075, img_1076, img_1077, img_1078, img_1079, img_1080, img_1081, img_1082, img_1083, img_1084, img_1085, img_1086, img_1087, img_1088, img_1089, img_1090, img_1091, img_1092, img_1093, img_1094, img_1095, img_1096, img_1097, img_1098, img_1099, img_1100, img_1101, img_1102, img_1103, img_1104, img_1105, img_1106, img_1107, img_1108, img_1109, img_1110, img_1111, img_1112, img_1113, img_1114, img_1115, img_1116, img_1117, img_1118, img_1119, img_1120, img_1121, img_1122, img_1123, img_1124, img_1125, img_1126, img_1127, img_1128, img_1129, img_1130, img_1131, img_1132, img_1133, img_1134, img_1135, img_1136, img_1137, img_1138, img_1139, img_1140, img_1141, img_1142, img_1143, img_1144, img_1145, img_1146, img_1147, img_1148, img_1149, img_1150, img_1151, img_1152, img_1153, img_1154, img_1155, img_1156, img_1157, img_1158, img_1159, img_1160, img_1161, img_1162, img_1163, img_1164, img_1165, img_1166, img_1167, img_1168, img_1169, img_1170, img_1171, img_1172, img_1173, img_1174, img_1175, img_1176, img_1177, img_1178, img_1179, img_1180, img_1181, img_1182, img_1183, img_1184, img_1185, img_1186, img_1187, img_1188, img_1189, img_1190, img_1191, img_1192, img_1193, img_1194, img_1195, img_1196, img_1197, img_1198, img_1199, img_1200, img_1201, img_1202, img_1203, img_1204, img_1205, img_1206, img_1207, img_1208, img_1209, img_1210, img_1211, img_1212, img_1213, img_1214, img_1215, img_1216, img_1217, img_1218, img_1219, img_1220, img_1221, img_1222, img_1223, img_1224, img_1225, img_1226, img_1227, img_1228, img_1229, img_1230, img_1231, img_1232, img_1233, img_1234, img_1235, img_1236, img_1237, img_1238, img_1239, img_1240, img_1241, img_1242, img_1243, img_1244, img_1245, img_1246, img_1247, img_1248, img_1249, img_1250, img_1251, img_1252, img_1253, img_1254, img_1255, img_1256, img_1257, img_1258, img_1259, img_1260, img_1261, img_1262, img_1263, img_1264, img_1265, img_1266, img_1267, img_1268, img_1269, img_1270, img_1271, img_1272, img_1273, img_1274, img_1275, img_1276, img_1277, img_1278, img_1279"
     ]
    }
   ],
   "source": [
    "target = train['Pawpularity']\n",
    "features = ['Subject Focus', 'Eyes', 'Face', 'Near', 'Action', 'Accessory',\n",
    "            'Group', 'Collage', 'Human', 'Occlusion', 'Info', 'Blur'] + [f\"img_{i}\" for i in np.arange(1280)]\n",
    "\n",
    "lgb_param = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'learning_rate': 0.01,\n",
    "    'seed': 42,\n",
    "    'max_depth': -1,\n",
    "    'min_data_in_leaf': 10,\n",
    "    'verbosity': -1,\n",
    "}\n",
    "\n",
    "oof, feature_importance_df, _ = run_kfold_lightgbm(lgb_param, \n",
    "                                                   train, features, target, \n",
    "                                                   n_fold=5, categorical=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_feature_importance(feature_importance_df)\n",
    "feature_importance_df.to_csv(OUTPUT_DIR+f'feature_importance_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデル出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['pred'] = oof\n",
    "score = get_RMSE(train['Pawpularity'].values, train['pred'].values)\n",
    "LOGGER.info(f\"CV: {score:<.5f}\")\n",
    "train[['Id', 'Pawpularity', 'pred']].to_pickle(OUTPUT_DIR+'oof.pkl')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
